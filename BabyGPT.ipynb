{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lauorie/Coursera-Machine-Learning/blob/main/BabyGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGIb09tW9veo"
      },
      "source": [
        "![image.png](https://huggingface.co/Laurie/HC3-Chinese-AlpacaFormat-Lora/resolve/main/BABY.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoCPNmZv9veq"
      },
      "source": [
        "# 构建数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEKfPsqR9ver"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/Laurie/qlora-v1/resolve/main/santi2.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYd51t0X9ves"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('/content/santi2.txt','r',encoding='gbk') as f:\n",
        "    text = f.read()\n",
        "print(\"文本长度：\",len(text),\"\\n\")\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6_7FK3f9veu"
      },
      "source": [
        "# 创建字典"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWL51Nl39veu"
      },
      "outputs": [],
      "source": [
        "words = list(set(text))\n",
        "words_size = len(words)\n",
        "print(\"词表：\",words,\"\\n\")\n",
        "print(\"词表的大小是：words_size =\",words_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4uLtD8e9vev"
      },
      "outputs": [],
      "source": [
        "word2idx = {k:v for v,k in enumerate(words)}\n",
        "print(\"字符对应数字：\",word2idx)\n",
        "\n",
        "idx2word = {k:v for k,v in enumerate(words)}\n",
        "print(\"\\n数字对应字符：\",idx2word)\n",
        "\n",
        "encode = lambda x : [word2idx[i] for i in x]  # 把字符编码成数字\n",
        "decode = lambda x : [idx2word[i] for i in x]  # 把数字解码为字符"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1wc_xng9vev"
      },
      "outputs": [],
      "source": [
        "input_test = \"面壁者罗辑\"\n",
        "\n",
        "print(encode(input_test))\n",
        "print(\"\\n\")\n",
        "print(decode(encode(input_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcTqDreH9vev"
      },
      "source": [
        "# 为进入Transformer作准备\n",
        "\n",
        "1. 进入Transformer首先要把输入格式转为Tensor格式\n",
        "\n",
        "2. 将数据拆分为训练集和验证集\n",
        "\n",
        "3. 进入的数据的长度是统一的，也就是句子的长度是统一的，即sequence_length参数\n",
        "\n",
        "4. 进入的数据是一批一批的，也就是几个句子一起进入，即batch_size参数\n",
        "\n",
        "5. 进入的数据是经过embedding的，也就是给它们嵌入维度，即d_model参数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slf0F-Ao9vew"
      },
      "source": [
        "## 转换tensor格式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN1HsW-w9vew"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text))  # 将数据转化为数字之后，再将数字转换为tensor格式\n",
        "\n",
        "print(data.shape, data.dtype)      # 打印数据的形状和数据类型，这时候的数据是矩阵样式的\n",
        "print(\"\\n\")\n",
        "print(data[:100]) # 打印前100来看看"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIpcGdKY9vew"
      },
      "source": [
        "## 拆分数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owm3lZxB9vew"
      },
      "outputs": [],
      "source": [
        "Split = int(0.8 * len(data)) # 我们设置8：2的训练集与验证集\n",
        "\n",
        "train_data = data[:Split]\n",
        "val_data = data[Split:]\n",
        "\n",
        "print(\"训练集大小：\", len(train_data))\n",
        "print(\"\\n验证集大小：\", len(val_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9tcT12e9vew"
      },
      "source": [
        "# 设置句子长度sequence_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_26mLwk9vex"
      },
      "outputs": [],
      "source": [
        "seq_len = 10\n",
        "\n",
        "x = train_data[:seq_len]           # featur特征\n",
        "y = train_data[1:seq_len + 1]      # target目标\n",
        "print('featur特征：', x)\n",
        "print('\\ntarget目标：', y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMFdSAsJ9vex"
      },
      "outputs": [],
      "source": [
        "for i in range(seq_len):\n",
        "    context = x[:i+1]\n",
        "    target = y[i]\n",
        "    print(f\"输入是 {context.tolist()}，预测值是 {target.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaaBzNyr9vex"
      },
      "outputs": [],
      "source": [
        "for i in range(seq_len):\n",
        "    context = x[:i+1]\n",
        "    target = int(y[i])\n",
        "    print(f'输入是   {\"\".join(decode(context.tolist()))}      预测字符是 {\"\".join(decode([target]))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ7ulwLb9vex"
      },
      "source": [
        "## 构造batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcN0quL19vex"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "batch_size = 4\n",
        "seq_len = 10\n",
        "\n",
        "def mini_batch(data):\n",
        "    idx = torch.randint(0, len(data) - seq_len, (batch_size,))    # 在0到len(data)-seq_len之间生成batch_size个随机数-->每个inputs的首位索引idx\n",
        "    inputs = torch.stack([data[i:i+seq_len] for i in idx])      # 将idx中的数作为索引，从data中取出seq_len个数，组成一个batch\n",
        "    targets = torch.stack([data[i+1:i+seq_len+1] for i in idx])   # 将idx中的数作为索引，从data中取出seq_len个数，组成一个batch\n",
        "    return inputs, targets\n",
        "\n",
        "inputs, targets = mini_batch(train_data)\n",
        "print(\"输入的形状：\", inputs.size(), \"\\n\", \"输入是：\",inputs)\n",
        "print(\"\\n\")\n",
        "print(\"目标的形状：\", targets.size(), \"\\n\", \"目标是：\",targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzTQOz0p9vey"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtoaqwOe9vey"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "torch.manual_seed(42)\n",
        "embedding_table = nn.Embedding(4, 4) # 4 words represents words_size, 4 dimensional embeddings\n",
        "embedding_table.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kK1f1xA9vey"
      },
      "outputs": [],
      "source": [
        "idx_input = torch.LongTensor(4,2).random_(0,4) # 输入的batch_size=4, seq_len=2，random_(0,4)表示随机生成words_size之间的整数\n",
        "\n",
        "print('查找的idx_input维度为:',idx_input.size())\n",
        "\n",
        "print('\\n查找的idx_input为：\\n', idx_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdugQTHW9vey"
      },
      "outputs": [],
      "source": [
        "target_emd = embedding_table(idx_input)  # 查找词向量\n",
        "print(\"查找到的词向量：\", target_emd, \"\\n\\n词向量的维度：\", target_emd.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUZsn3RT9vey"
      },
      "source": [
        "## 为什么要用Embedding呢？\n",
        "\n",
        "Embedding video: https://youtu.be/W_ZUUDJsUtA\n",
        "\n",
        "![image](https://huggingface.co/Laurie/HC3-Chinese-AlpacaFormat-Lora/resolve/main/embedding.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36-1ypY59vez"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class BabyGPT(nn.Module):\n",
        "    def __init__(self, words_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(words_size, words_size) # the last words_size is the embedding dimension, which can be different from the words_size like 512, 1024, etc.\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is a tensor of shape (batch_size, seq_len)\n",
        "        # targets is a tensor of shape (batch_size, seq_len)\n",
        "        logits = self.embedding(idx)  # (batch_size, seq_len, embedding_dim)\n",
        "        if targets is None:\n",
        "            loss = None  # compute the loss if target is not None\n",
        "        else:\n",
        "            B, S, E = logits.shape\n",
        "            logits = logits.view(B*S, E)     # (B*S, E)\n",
        "            targets = targets.view(B*S)      # (B*S) reshape targets to match logits\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # 以自回归的方式生成新的token,(B, S) -> (B, S+1) -> (B, S+2) -> ... -> (B, S+max_new_tokens)\n",
        "        for _ in range(max_new_tokens):\n",
        "            # generate new tokens\n",
        "            logits, loss = self.forward(idx)\n",
        "            # only take the last token\n",
        "            logits = logits[:, -1, :]  # (B, E) take the last token,see explanation below\n",
        "            # apply softmax to convert to probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, E) the sum of probs is 1, 使得到的embedding vector之和为1，属于归一化的一种方式,see explanation below\n",
        "            # sample from the distribution or take the most likely\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1) 从probs概率分布中随机采样，采样结果的probs值对应的index，它的shape为(B, 1)\n",
        "            # append to the sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=-1) # (B, S+1) concatenate the new token to the sequence until max_new_tokens\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lmjIzX-9vez"
      },
      "source": [
        "### 解释：\n",
        "1. logits = logits[:, -1, :]\n",
        "\n",
        "2. probs = F.softmax(logits, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "5DhnApx_9vez"
      },
      "outputs": [],
      "source": [
        "1. logits = logits[:, -1, :]\n",
        "假设我们输入一个句子序列,经过Transformer编码器,得到的logits张量形状是(batch_size=2, seq_len=5, hidden_size=128):\n",
        "\n",
        "logits =\n",
        "[[[1.1, 1.2, ..., 1.128],   # 我\n",
        "  [2.1, 2.2, ..., 2.128],   # 爱\n",
        "  [3.1, 3.2, ..., 3.128],   # 你\n",
        "  [4.1, 4.2, ..., 4.128],   # 中\n",
        "  [5.1, 5.2, ..., 5.128]],  # 国\n",
        "\n",
        " [[6.1, 6.2, ..., 6.128],     # I\n",
        "  [7.1, 7.2, ..., 7.128],     # love\n",
        "  [8.1, 8.2, ..., 8.128],     # my\n",
        "  [9.1, 9.2, ..., 9.128],     # hometown\n",
        "  [10.1, 10.2, ..., 10.128]]] # China\n",
        "\n",
        "  * batch_size,可以看到它包含2个句子\n",
        "  * seq_len,矩阵的竖向长度,也是句子的长度为5\n",
        "  * hidden_size,隐状态,也即embedding维度\n",
        "\n",
        "这个logits包含了2个句子样本,每个句子长度为5,每个时间步是一个128维的向量表示该时刻的隐状态。\n",
        "\n",
        "现在我们取最后一个时间步/token的向量,即取出:\n",
        "\n",
        "  logits = logits[:, -1, :]\n",
        "\n",
        "  [5.1, 5.2, ..., 5.128]    # 国\n",
        "  [10.1, 10.2, ..., 10.128] # China\n",
        "\n",
        "那么就是取出了每个句子的最后一个token的隐状态,这个隐状态就是整个句子的向量表示,它的形状是(batch_size=2, hidden_size=128):\n",
        "last_hidden = logits[:, -1, :]\n",
        "# last_hidden 的形状是 (2, 128)\n",
        "\n",
        "2. probs = F.softmax(logits, dim=-1)\n",
        "即将下面两个向量进行softmax:\n",
        "  [5.1, 5.2, ..., 5.128]    # 国，里面的值经过softmax后加起来是1\n",
        "  [10.1, 10.2, ..., 10.128] # China，里面的值经过softmax后加起来是1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIYmiLgM9vez"
      },
      "source": [
        "# 构建模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciKCjKv59vez"
      },
      "outputs": [],
      "source": [
        "model = BabyGPT(words_size)\n",
        "logits, loss = model(inputs, targets)\n",
        "print(logits.shape, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDFQK_Rh9vez"
      },
      "outputs": [],
      "source": [
        "input_word = '罗'\n",
        "\n",
        "input_word_encoded = encode(input_word)\n",
        "input_word_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs4OOxui9ve0"
      },
      "outputs": [],
      "source": [
        "idx = torch.tensor([input_word_encoded],dtype=torch.long)\n",
        "\n",
        "idx_pred = model.generate(idx,max_new_tokens=10)\n",
        "print(\"预测的词的索引为：\",idx_pred,\"\\n维度为：\",idx_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br2KN8MP9ve0"
      },
      "outputs": [],
      "source": [
        "print(\"\".join(decode(idx_pred[0].tolist()))) # 将预测结果解码为文本"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LT_ApoRgNp7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JToDbq2hNpzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w6yBbjSrNpo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fcLEkLmkNpd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[panda](https://img.aidotu.com/down/jpg/20201220/2db7babbc57b0b05b4c43ea6da0f13d6_5929_200_186.jpg)"
      ],
      "metadata": {
        "id": "n2XeeFiYMsgf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnojuZOh9ve0"
      },
      "source": [
        "# 创建优化器\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmWVZF9S9ve0"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2) # 优化器的作用是更新参数，使得损失函数最小"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztuItESa9ve0"
      },
      "source": [
        "# 模型训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6nqb_eQ9ve0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "losses = []  # List to store the loss values\n",
        "\n",
        "batch_size = 128\n",
        "seq_length = 64\n",
        "for step in range(1000):\n",
        "    # 利用训练集进行训练\n",
        "    inputs, targets = mini_batch(train_data)\n",
        "    logits, loss = model(inputs, targets)\n",
        "    optimizer.zero_grad(set_to_none=True)  # 梯度清零,set_to_none=True表示不会占用额外的内存\n",
        "    loss.backward()  # 反向传播计算梯度\n",
        "    optimizer.step()  # 更新参数\n",
        "\n",
        "    losses.append(loss.item())  # Store the loss value\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print('Step {} Loss {:.4f}'.format(step, loss))\n",
        "\n",
        "# Plot the loss graph\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Graph')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[](https://media.tenor.com/KMxrZ-A6ev4AAAAC/nice-smack.gif)"
      ],
      "metadata": {
        "id": "HjWdaUS7Oe7e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8CvCV0k9ve1"
      },
      "outputs": [],
      "source": [
        "print(\"\".join(decode(model.generate(torch.tensor([input_word_encoded],dtype=torch.long),max_new_tokens=10)[0].tolist())))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B_evlzxWPAyo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQw4B4d39ve2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 128\n",
        "seq_len = 64\n",
        "max_iters = 3000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "# data\n",
        "torch.manual_seed(42)\n",
        "# !wget https://huggingface.co/Laurie/qlora-v1/resolve/main/santi2.txt\n",
        "with open('santi2.txt', 'r', encoding='gbk') as f:\n",
        "    text = f.read()\n",
        "words = sorted(list(set(text)))\n",
        "words_size = len(words)\n",
        "stoi = { ch:i for i,ch in enumerate(words) }\n",
        "itos = { i:ch for i,ch in enumerate(words) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# trian/val split\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "Split = int(.8*len(data))\n",
        "train_data = data[:Split]\n",
        "val_data = data[Split:]\n",
        "\n",
        "# get a batch of data\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
        "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class BabyGPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(words_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(seq_len, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, words_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,words_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last seq_len tokens\n",
        "            idx_cond = idx[:, -seq_len:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BabyGPT()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(\"模型共有：\",sum(p.numel() for p in m.parameters())/1e5, '* 十万参数')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        train_loss = losses['train']\n",
        "        val_loss = losses['val']\n",
        "        print(f\"step {iter}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# Plotting the loss curve\n",
        "plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
        "plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "input_test = \"罗辑\"   # 把这里的内容改成你想要的内容\n",
        "input_word_encoded = encode(input_test)\n",
        "context = torch.tensor([input_word_encoded],device=device,dtype=torch.long)\n",
        "print(decode(m.generate(context, max_new_tokens=100)[0].tolist())) # 把这里的数字改成你想要生成文本的长度"
      ],
      "metadata": {
        "id": "ypzCGXXxSqoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练LLaMA2模型的数据量大概等于多少本三体Ⅱ？\n",
        "\n",
        "根据网上的信息,我大概估算了一下LLaMA2模型使用的数据量相当于多少本《三体II:黑暗森林》的词数:\n",
        "\n",
        "1. LLaMA2模型使用了初代LLaMA模型预训练好的137亿参数,然后在此基础上继续 pretrain,增加了460亿参数,总参数量达到了597亿。\n",
        "\n",
        "2. 三体II《黑暗森林》小说全文大约有40万字。\n",
        "\n",
        "3. 如果按照每个中文词平均2字计算,40万字约等于20万词。\n",
        "\n",
        "4. 597亿参数除以20万词,约等于**3000亿个三体II《黑暗森林》**的词数。\n",
        "\n",
        "所以一个非常粗略的估计,LLaMA2模型使用的训练数据量,大概相当于3000亿本《三体II:黑暗森林》的训练词数。\n",
        "当然这是非常粗略的估计,可能存在很大的误差。但可以看出LLaMA2使用的训练数据量级非常大,达到了数以万亿计的量级。这也使得LLaMA2模型获得了很强的语言理解和生成能力。"
      ],
      "metadata": {
        "id": "KhkG3VYsSFIL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A3-7Ml89ve2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urbWPqOm9ve3"
      },
      "source": [
        "# Gradient Descent\n",
        "https://www.jiqizhixin.com/articles/2019-04-07-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fom0Fpph9ve3"
      },
      "source": [
        "# Loss and Learning Rate\n",
        "https://developers.google.com/machine-learning/crash-course/fitter/graph?hl=zh-cn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiJq-NO39ve3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-wdU_N19ve3"
      },
      "source": [
        "# Temperature & Top_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke7Om6JH9ve3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "test_tensor =  torch.tensor([-1.2345, -0.0431, -1.6047, -0.7521, -0.6866])\n",
        "test_tensor_softmax = F.softmax(test_tensor, dim=0)\n",
        "test_tensor_softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNlkMN3H9ve3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_test = test_tensor_softmax.tolist()\n",
        "\n",
        "plt.bar(range(len(data_test)), data_test)\n",
        "\n",
        "# 在柱子上添加数值标签\n",
        "for x, y in enumerate(data_test):\n",
        "    plt.text(x, y, str(round(y,3)), ha='center')\n",
        "\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "plt.title('somftmax vannila')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4E0gXEg9ve4"
      },
      "outputs": [],
      "source": [
        "test_tensor1 =  torch.tensor([-.12345, -.00431, -.16047, -.07521, -.06866])\n",
        "test_tensor_softmax1 = F.softmax(test_tensor1, dim=0)\n",
        "test_tensor_softmax1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcZIS78Z9ve4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = test_tensor_softmax1.tolist()\n",
        "\n",
        "plt.bar(range(len(data)), data)\n",
        "\n",
        "# 在柱子上添加数值标签\n",
        "for x, y in enumerate(data):\n",
        "    plt.text(x, y, str(round(y,3)), ha='center')\n",
        "\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "plt.title('soft softmax')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSroVvPD9ve4"
      },
      "source": [
        "# Label Smoothing\n",
        "主要步骤是:\n",
        "\n",
        "1. 对每个样本,创建一个全0的软标签分布\n",
        "\n",
        "2. 为正确类保留1-ε的概率\n",
        "\n",
        "3. 均分ε到其他类\n",
        "\n",
        "4. 得到软标签分布作为新目标,用于训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqiRKwAO9ve4"
      },
      "outputs": [],
      "source": [
        "# 原始数据\n",
        "y_true = [1, 0, 0, 1, 1, 2] # 独热编码的真实标签\n",
        "num_classes = 3 # 类别数量\n",
        "\n",
        "# Label Smoothing\n",
        "smoothed_labels = []\n",
        "smoothing_epsilon = 0.1\n",
        "\n",
        "for i in range(len(y_true)):\n",
        "  smoothed_label = [0] * num_classes\n",
        "\n",
        "  for j in range(num_classes):\n",
        "    if j == y_true[i]:\n",
        "      smoothed_label[j] = 1 - smoothing_epsilon\n",
        "    else:\n",
        "      smoothed_label[j] = smoothing_epsilon / (num_classes - 1)\n",
        "\n",
        "  smoothed_labels.append(smoothed_label)\n",
        "\n",
        "# 训练模型,使用smoothed_labels作为目标\n",
        "# model.train(X, smoothed_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrL-xRKD9ve4"
      },
      "outputs": [],
      "source": [
        "smoothed_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMCWOg_H9ve5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_lmjIzX-9vez"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}